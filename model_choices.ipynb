{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats, optimize\n",
    "from pandas import DataFrame, Series\n",
    "import seaborn as sns\n",
    "import random as rd\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import scipy.stats\n",
    "import patsy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import basinhopping\n",
    "from sklearn import linear_model\n",
    "import multiprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrials = 100\n",
    "nbandits = 9\n",
    "choices = np.random.randint(nbandits, size=(ntrials+1))\n",
    "outcomes = (np.random.randint(3, size=(ntrials+1))-1)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##compute RL values for a given alpha\n",
    "def perform_RL(choices,outcomes,alpha):    \n",
    "    V = np.zeros((ntrials+1,nbandits)) #initialize V\n",
    "    \n",
    "    if len(alpha) == 2: #seperate alphas for gains and losses\n",
    "        a1,a2 = alpha\n",
    "        alpha_flag = True\n",
    "    else:\n",
    "        alpha = alpha[0]\n",
    "        alpha_flag = False    \n",
    "    \n",
    "    for i in range(1,ntrials+1): \n",
    "        for j in range(nbandits):\n",
    "\n",
    "            if j == choices[i]: #chose this bandit\n",
    "                delta = outcomes[i] - V[i-1,bandit] #PE\n",
    "                \n",
    "                if alpha_flag == True and outcomes[i] >= 0: #gain\n",
    "                    V[i,j] = V[i-1,j] + a1 * delta #update\n",
    "                elif alpha_flag == True and outcomes[i] < 0: #loss\n",
    "                    V[i,j] = V[i-1,j] + a2 * delta #update\n",
    "                else:\n",
    "                    V[i,j] = V[i-1,j] + alpha* delta #update\n",
    "\n",
    "            else: #no choice, just update value\n",
    "                V[i,j] = V[i-1,j]\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#computes subjective utility for each subjects choices\n",
    "def subjective_utility(outcomes,l,rho):\n",
    "    su = []\n",
    "    for o in outcomes:\n",
    "        if o >=0 :\n",
    "            su.append(o**rho)\n",
    "        else:\n",
    "            su.append((-1*l) * (-1*o)**rho)\n",
    "    return su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(values,choice_index,beta):\n",
    "    values = np.exp(values*beta)\n",
    "    p_choice = values[choice_index]/np.sum(values)\n",
    "    return p_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compute loss for RL model\n",
    "def choice_likelihood(params,choices,outcomes,seperate_alphas):\n",
    "    \n",
    "    if seperate_alphas:\n",
    "        alpha1 = scipy.stats.logistic.cdf(params[0]) #learning rate\n",
    "        alpha2 = scipy.stats.logistic.cdf(params[1]) #learning rate\n",
    "        beta = np.log(params[2])    \n",
    "        V = perform_RL(choices,outcomes,(alpha1,alpha2)) #get values from RL         \n",
    "    else:        \n",
    "        alpha = scipy.stats.logistic.cdf(params[0]) #learning rate\n",
    "        beta = np.log(params[1])    \n",
    "        V = perform_RL(choices,outcomes,(alpha,)) #get values from RL \n",
    "        \n",
    "    #compute LL under softmax\n",
    "    ll = 0\n",
    "    for i in range(1,ntrials+1):\n",
    "        p_choice = softmax(V[i-1,:],choices[i],beta)\n",
    "        ll += np.log(p_choice)\n",
    "        \n",
    "    ll = ll*-1 #minimize neg LL\n",
    "    \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 219.722458\n",
      "         Iterations: 1\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 3\n",
      "alpha for gains is 1.95953849504e-14\n",
      "alpha for losses is 3.1431888221e-29\n",
      "beta is 6.65991330117\n"
     ]
    }
   ],
   "source": [
    "compute_su = False #compute estimates with subjective utilities instead of objective outcomes\n",
    "seperate_alphas = True #compute seperate learning rates for gains and losses\n",
    "\n",
    "#transform outcomes if you want to compute subjective utilities\n",
    "if compute_su:\n",
    "    outcomes = subjective_utility(outcomes,l,rho)\n",
    "\n",
    "#initialize parameters\n",
    "if seperate_alphas:\n",
    "    params = [.1,.1,.1] #initialize alpha and beta\n",
    "else:\n",
    "    params = [.1,.1] #initialize alpha and beta\n",
    "\n",
    "#run minimization\n",
    "res = minimize(choice_likelihood,params,args=(choices,outcomes,seperate_alphas),method='BFGS', options={'disp': True})\n",
    "\n",
    "#examine results\n",
    "if seperate_alphas:\n",
    "    alpha1 = scipy.stats.logistic.cdf(res.x[0]) #learning rate\n",
    "    alpha2 = scipy.stats.logistic.cdf(res.x[1]) #learning rate\n",
    "    beta = np.log(res.x[2])\n",
    "    print \"alpha for gains is \" + str(alpha1)\n",
    "    print \"alpha for losses is \" + str(alpha2)\n",
    "    print \"beta is \" + str(beta)   \n",
    "else:\n",
    "    alpha = scipy.stats.logistic.cdf(res.x[0]) #learning rate\n",
    "    beta = np.log(res.x[1])\n",
    "    print \"alpha is \" + str(alpha)\n",
    "    print \"beta is \" + str(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
